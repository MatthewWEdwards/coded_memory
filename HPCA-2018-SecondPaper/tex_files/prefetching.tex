%{\color{blue} \noindent \textbf{Potential improvement using prefetching:}~
%\Ethan{Remove entire subsection?}
%\noindent \textbf{Future Work--Prefetching:}~
\Matt{Is prefetching worth bringing up? Shouldn't this be somewhere before the conclusion?} One potential source of improvement is the addition of a memory prefetching unit, similar to an instruction or cache prefetching unit, which can detect linear access patterns to memory regions. For example, if a sequence of memory accesses is issued in increasing order spaced one byte apart, then a prefetching unit would predict the next access to be one byte past the previous one. 
Prefetching-based memory designs have been studied only in the context of {\em uncoded} memory systems~\cite{Kim2016, Kadjo2014, Shevgoor2015, JL2013}. We can augment our scheme by fetching a predicted address from a parity bank during accesses for which it remains valid but idle. Then for future memory accesses, the controller can check the prefetched data and attempt to complete the request using current accesses and prefetched data. This means that previously occupied banks are available to serve more accesses per cycle. As memory accesses wait to be issued in the bank queues, they can simultaneously be checked with prefetched data. Thus, no extra latency is anticipated by the addition of a memory prefetching unit.
%
Figure~\ref{fig:prefetch1} shows two plots of memory accesses for several banks across time. Both figures suggest linear access patterns and thus larger performance improvements when coding caching is combined with prefetching for this application.

%
%To the best of our knowledge, the application of prefetching schemes with coded memory systems has not been explored before.
%
%For example, say the prefetcher sees 2 consecutive memory requests in a row. It then predicts that the next two accesses, locations $a_0$ and $b_0$, are likely to be accessed in the near future. It reads $a_0 + b_0$ from the parity bank for future use. Next, access to location $a_0$ and $b_0$ are issued to the memory. Now, instead of reading both $a_0$ and $b_0$, only a single location has to be read from in memory, while the other location can be obtained from the prefetched data. This allows for an additional access to be issued from the now free memory bank.  In these cases, it is possible to obtain up to two additional memory accesses in a given cycle, one from the prefetched data and one from the parity bank.
%
%Implementation of a memory prefetch should only require overhead for space and the associated logic to implement it. Since memory accesses are often stalled due to bank conflicts, checking pending accesses to the prefetched data should require no additional time overhead. 
%
%\noindent {\bf Coding and prefetching:~}One of the ideas explored in this paper is that of proactive prefetching of data from (unused) memory banks to buffers based on the pattern of pending access requests at the memory controller. This creates the opportunities to serve multiple access requests in a given memory cycles and also tries to utilize all the available memory banks throughout the operation of the memory system. Such prefetching based memory designs in the context of {\em uncoded} memory systems has been previously studied in the literature (see e.g.,~\cite{Kim2016, Kadjo2014, Shevgoor2015, JL2013}). To the best of our knowledge, the application of prefetching schemes with coded memory systems has not been explored before.