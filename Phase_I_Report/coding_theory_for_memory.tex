Coding theory is the study of codes and their applications to specific fields. Coding has been used in a variety of computer science applications, from error correcting in the transmitting of data to increased data storage compression. We aim to extend the benefits of coding theory to dynamic random-access memory systems. We propose a memory scheme in which a small portion of memory is reserved for the efficient coding of pre-existing data. In essence, this allows the data of one bank to be duplicated and stored in an additional memory location. Traditionally, when multiple requests to a single bank are issued by the processor, a stall is generated. These types of stalls, known as bank conflicts, result from the fact that only one address from a single bank can be accessed at a time. The processor must wait for the result from the first bank access to return before it can serve additional requests to the same bank. This lag can be a major bottleneck in a computer's processing speed. With a coded memory scheme, data present in multiple data banks will be compressed and stored in extra banks, known as a parity banks. These parity banks will then be accessed concurrently with corresponding data banks to help alleviate stalls from bank conflicts. Ultimately, with the addition of a single parity bank we are able to generate a single additional access to any arbitrary bank without implementing any further logic to the bank itself.
In the following sections, we first describe the design parameters used to design the coding system. We then describe the two coding designs we have implemented, and give the initial results we have achieved through the implementation of these designs. We also describe certain techniques to reduce the cost of coding implementation.

